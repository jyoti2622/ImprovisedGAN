{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab721541-15c1-4e8c-84f2-a12d50b37c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:41:50\n"
     ]
    }
   ],
   "source": [
    "#This is the class for encoder \n",
    "#This is the class for encoder \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_features,embedding_dim=16,device=None):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.in_dim=n_features\n",
    "        self.device=device\n",
    "        \n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "        self.lstm1 = nn.LSTM(\n",
    "                    input_size=n_features,\n",
    "                    hidden_size=self.hidden_dim,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True\n",
    "                    )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "                    input_size=self.hidden_dim,\n",
    "                    hidden_size=embedding_dim,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True\n",
    "                    )\n",
    "       \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        batch_size,seq_len=input.size(0),input.size(1)\n",
    "        \n",
    "        #h is the hidden state at time t and c is the cell state at time t\n",
    "        h_0 = torch.zeros(1, batch_size, 2 * self.embedding_dim).to(self.device)\n",
    "        c_0 = torch.zeros(1, batch_size, 2 * self.embedding_dim).to(self.device)\n",
    "        \n",
    "        recurrent_features,(h_1,c_1) = self.lstm1(input,(h_0,c_0))\n",
    "        recurrent_features,_ = self.lstm2(recurrent_features)\n",
    "        \n",
    "        outputs=recurrent_features.view(batch_size,seq_len,self.embedding_dim)\n",
    "        \n",
    "        return outputs,recurrent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465d69a1-bfe3-4da1-a872-540129c40af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is also the Generator\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"An LSTM based generator. It expects a sequence of noise vectors as input.\n",
    "    Args:\n",
    "        in_dim: Input noise dimensionality\n",
    "        out_dim: Output dimensionality\n",
    "        n_layers: number of lstm layers\n",
    "        hidden_dim: dimensionality of the hidden layer of lstms\n",
    "    Input: noise of shape (batch_size, seq_len, in_dim)\n",
    "    Output: sequence of shape (batch_size, seq_len, out_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, device=None):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm0 = nn.LSTM(in_dim, hidden_size=32, num_layers=1, batch_first=True)\n",
    "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Sequential(nn.Linear(in_features=128, out_features=out_dim), nn.Tanh())\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len = input.size(0), input.size(1)\n",
    "        h_0 = torch.zeros(1, batch_size, 32).to(self.device)\n",
    "        c_0 = torch.zeros(1, batch_size, 32).to(self.device)\n",
    "\n",
    "        recurrent_features, (h_1, c_1) = self.lstm0(input, (h_0, c_0))\n",
    "        recurrent_features, (h_2, c_2) = self.lstm1(recurrent_features)\n",
    "        recurrent_features, _ = self.lstm2(recurrent_features)\n",
    "        \n",
    "        outputs = self.linear(recurrent_features.contiguous().view(batch_size*seq_len, 128))\n",
    "        outputs = outputs.view(batch_size, seq_len, self.out_dim)\n",
    "        return outputs, recurrent_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23e14e6-179a-45a9-ab46-2e76524ba5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self,in_dim,device=None):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "\n",
    "        self.lstm=nn.LSTM(input_size=in_dim,hidden_size=100,num_layers=1,batch_first=True)\n",
    "        self.linear = nn.Sequential(nn.Linear(100,1))\n",
    "\n",
    "    def forward(self,input):\n",
    "        batch_size,seq_len = input.size(0), input.size(1)\n",
    "        h_0 = torch.zeros(1, batch_size, 100).to(self.device)\n",
    "        c_0 = torch.zeros(1, batch_size, 100).to(self.device)\n",
    "\n",
    "        recurrent_features, _ = self.lstm(input, (h_0, c_0))\n",
    "        outputs = self.linear(recurrent_features.contiguous().view(batch_size*seq_len, 100))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e6c205-ca17-43a9-aee2-65941f37ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_chan_name(chan_list_str):\n",
    "    if len(chan_list_str) > 2:\n",
    "        chan_list_str = chan_list_str[2:-2]\n",
    "        chan_list = chan_list_str.split(\"', '\")\n",
    "        return chan_list\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "class Wadi(Dataset,):\n",
    "    def __init__(self,seed: int,data_settings=None,remove_unique=False, entity=None, verbose=False, one_hot=False):\n",
    "        super().__init__()\n",
    "        self._data = None\n",
    "        self.one_hot=one_hot\n",
    "        self.verbose=verbose\n",
    "        self.remove_unique=remove_unique\n",
    "        self.train=data_settings.train\n",
    "        self.dataset_training_name =data_settings.dataset_training_name\n",
    "        self.dataset_test_name =data_settings.dataset_test_name\n",
    "        self.dataset_anomaly_name =data_settings.dataset_anomaly_name\n",
    "        self.window_length=data_settings.window_length\n",
    "        if data_settings.train:\n",
    "            self.stride=1\n",
    "        else:\n",
    "            self.stride=self.window_length\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        x,y=self.load()\n",
    "        self.n_feature = len(x.columns)\n",
    "        x,y=self.unroll(x[:1000],y[:1000])\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(np.array([1 if sum(y_i) > 0 else 0 for y_i in y])).float()\n",
    "        \n",
    "        self.data_len = x.shape[0]  \n",
    "       '''\n",
    "    def data(self) -> (pd.DataFrame, pd.Series, pd.DataFrame, pd.Series):\n",
    "        \"\"\"Return data, load if necessary\"\"\"\n",
    "        if self._data is None:\n",
    "            self.load()\n",
    "        return self._data    \n",
    "         \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def standardize(self,X_train, X_test, remove=False, verbose=False, max_clip=5, min_clip=-4):\n",
    "\n",
    "        mini = X_train.min()\n",
    "        maxi = X_train.max()\n",
    "        for col in X_train.columns:\n",
    "            if maxi[col] != mini[col]:\n",
    "                X_train[col] = (X_train[col] - mini[col]) / (maxi[col] - mini[col])\n",
    "                X_test[col] = (X_test[col] - mini[col]) / (maxi[col] - mini[col])\n",
    "                X_test[col] = np.clip(X_test[col], a_min=min_clip, a_max=max_clip)\n",
    "            else:\n",
    "                assert X_train[col].nunique() == 1\n",
    "                if remove:\n",
    "                    if verbose:\n",
    "                        print(\"Column {} has the same min and max value in train. Will remove this column\".format(col))\n",
    "                    X_train = X_train.drop(col, axis=1)\n",
    "                    X_test = X_test.drop(col, axis=1)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"Column {} has the same min and max value in train. Will scale to 1\".format(col))\n",
    "                    if mini[col] != 0:\n",
    "                        X_train[col] = X_train[col] / mini[col]  # Redundant operation, just for consistency\n",
    "                        X_test[col] = X_test[col] / mini[col]\n",
    "                    if verbose:\n",
    "                        print(\"After transformation, train unique vals: {}, test unique vals: {}\".format(\n",
    "                        X_train[col].unique(),\n",
    "                        X_test[col].unique()))\n",
    "        return X_train, X_test\n",
    "\n",
    "    def format_data(self, train_df, test_df, OUTLIER_CLASS=1, verbose=False):\n",
    "        train_only_cols = set(train_df.columns).difference(set(test_df.columns))\n",
    "        if verbose:\n",
    "            print(\"Columns {} present only in the training set, removing them\")\n",
    "        train_df = train_df.drop(train_only_cols, axis=1)\n",
    "\n",
    "        test_only_cols = set(test_df.columns).difference(set(train_df.columns))\n",
    "        if verbose:\n",
    "            print(\"Columns {} present only in the test set, removing them\")\n",
    "        test_df = test_df.drop(test_only_cols, axis=1)\n",
    "\n",
    "        train_anomalies = train_df[train_df[\"y\"] == OUTLIER_CLASS]\n",
    "        test_anomalies: pd.DataFrame = test_df[test_df[\"y\"] == OUTLIER_CLASS]\n",
    "        print(\"Total Number of anomalies in train set = {}\".format(len(train_anomalies)))\n",
    "        print(\"Total Number of anomalies in test set = {}\".format(len(test_anomalies)))\n",
    "        print(\"% of anomalies in the test set = {}\".format(len(test_anomalies) / len(test_df) * 100))\n",
    "        print(\"number of anomalous events = {}\".format(len(self.get_events(y_test=test_df[\"y\"].values,outlier=1,normal=0))))\n",
    "        # Remove the labels from the data\n",
    "        X_train = train_df.drop([\"y\"], axis=1)\n",
    "        y_train = train_df[\"y\"]\n",
    "        X_test = test_df.drop([\"y\"], axis=1)\n",
    "        y_test = test_df[\"y\"]\n",
    "        self.y_test = y_test\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]  \n",
    "    \n",
    "    def get_events(self,y_test, outlier=1, normal=0, breaks=[]):\n",
    "        events = dict()\n",
    "        label_prev = normal\n",
    "        event = 0  # corresponds to no event\n",
    "        event_start = 0\n",
    "        for tim, label in enumerate(y_test):\n",
    "            if label == outlier:\n",
    "                if label_prev == normal:\n",
    "                    event += 1\n",
    "                    event_start = tim\n",
    "                elif tim in breaks:\n",
    "                    # A break point was hit, end current event and start new one\n",
    "                    event_end = tim - 1\n",
    "                    events[event] = (event_start, event_end)\n",
    "                    event += 1\n",
    "                    event_start = tim\n",
    "\n",
    "            else:\n",
    "                # event_by_time_true[tim] = 0\n",
    "                if label_prev == outlier:\n",
    "                    event_end = tim - 1\n",
    "                    events[event] = (event_start, event_end)\n",
    "            label_prev = label\n",
    "\n",
    "        if label_prev == outlier:\n",
    "            event_end = tim - 1\n",
    "            events[event] = (event_start, event_end)\n",
    "        return events\n",
    "        \n",
    "    def load(self):\n",
    "        OUTLIER_CLASS = 1\n",
    "        df_x_train: pd.DataFrame = pd.read_csv(self.dataset_training_name, header=3)\n",
    "        df_x_test: pd.DataFrame = pd.read_csv(self.dataset_test_name, header=0)\n",
    "        # Removing 4 columns who only contain nans (data missing from the csv file)\n",
    "        nan_columns = [r'\\\\WIN-25J4RO10SBF\\LOG_DATA\\SUTD_WADI\\LOG_DATA\\2_LS_001_AL',\n",
    "                       r'\\\\WIN-25J4RO10SBF\\LOG_DATA\\SUTD_WADI\\LOG_DATA\\2_LS_002_AL',\n",
    "                       r'\\\\WIN-25J4RO10SBF\\LOG_DATA\\SUTD_WADI\\LOG_DATA\\2_P_001_STATUS',\n",
    "                       r'\\\\WIN-25J4RO10SBF\\LOG_DATA\\SUTD_WADI\\LOG_DATA\\2_P_002_STATUS']        \n",
    "        df_x_train=df_x_train.drop(nan_columns, axis=1)\n",
    "        df_x_test = df_x_test.drop(nan_columns, axis=1)\n",
    "        \n",
    "        df_x_train = df_x_train.rename(columns={col: col.split('\\\\')[-1] for col in df_x_train.columns})\n",
    "        df_x_test = df_x_test.rename(columns={col: col.split('\\\\')[-1] for col in df_x_test.columns})\n",
    "        \n",
    "        \n",
    "        print(df_x_train.shape)\n",
    "        ano_df = pd.read_csv(self.dataset_anomaly_name, header=0)\n",
    "        df_x_train[\"y\"] = np.zeros(df_x_train.shape[0])\n",
    "        df_x_test[\"y\"] = np.zeros(df_x_test.shape[0])\n",
    "        \n",
    "        pd.set_option('mode.chained_assignment', None) # This is to prevent error SettingWithCopyWarning:A value is trying to be set on a copy of a slice from a DataFrame\n",
    "        for i in range(ano_df.shape[0]):\n",
    "            ano = ano_df.iloc[i, :][[\"Start_time\", \"End_time\", \"Date\"]]\n",
    "            start_row = np.where((df_x_test[\"Time\"].values == ano[\"Start_time\"]) &\n",
    "                                 (df_x_test[\"Date\"].values == ano[\"Date\"]))[0][0]\n",
    "            end_row = np.where((df_x_test[\"Time\"].values == ano[\"End_time\"]) &\n",
    "                               (df_x_test[\"Date\"].values == ano[\"Date\"]))[0][0]\n",
    "            df_x_test[\"y\"].iloc[start_row:(end_row + 1)] = np.ones(1 + end_row - start_row)\n",
    "            \n",
    "        df_x_train = df_x_train.drop([\"Time\", \"Date\", \"Row\"], axis=1)\n",
    "        df_x_test = df_x_test.drop([\"Time\", \"Date\", \"Row\"], axis=1)\n",
    "     \n",
    "        if self.one_hot:\n",
    "            # actuator colums (categoricals) with < 2 categories (all of these have 3 categories)\n",
    "            one_hot_cols = ['1_MV_001_STATUS', '1_MV_002_STATUS', '1_MV_003_STATUS', '1_MV_004_STATUS', '2_MV_003_STATUS',\n",
    "                            '2_MV_006_STATUS', '2_MV_101_STATUS', '2_MV_201_STATUS', '2_MV_301_STATUS', '2_MV_401_STATUS',\n",
    "                            '2_MV_501_STATUS', '2_MV_601_STATUS']\n",
    "\n",
    "            # combining before encoding because some categories only seen in test\n",
    "            one_hot_encoded = Dataset.one_hot_encoding(pd.concat([df_x_train, df_x_test], axis=0, join=\"inner\"),\n",
    "                                                       col_names=one_hot_cols)\n",
    "            df_x_train = one_hot_encoded.iloc[:len(df_x_train)]\n",
    "            df_x_test = one_hot_encoded.iloc[len(df_x_train):]\n",
    "        \n",
    "        \n",
    "        X_train, y_train, X_test, y_test = self.format_data(df_x_train, df_x_test, OUTLIER_CLASS, verbose=self.verbose)\n",
    "        X_train, X_test = self.standardize(X_train, X_test)\n",
    "        \n",
    "        \n",
    "        #X_train,y_train=self.unroll(X_train,y_train)\n",
    "        \n",
    "        #self.train=0\n",
    "        #self.stride=self.window_length\n",
    "        \n",
    "        #X_test,y_test=self.unroll(X_test,y_test)\n",
    "        self._data = tuple([X_train, y_train, X_test, y_test])\n",
    "        #return X_train,y_train,X_test,y_test\n",
    "        \n",
    "    def unroll(self, data, labels):\n",
    "        un_data = []\n",
    "        un_labels = []\n",
    "        seq_len = int(self.window_length)\n",
    "        stride = int(self.stride)\n",
    "        \n",
    "        idx = 0\n",
    "        while(idx < len(data) - seq_len):\n",
    "            \n",
    "            if self.train==1 and labels.loc[idx]==1.0:\n",
    "              idx += stride\n",
    "              continue\n",
    "            un_data.append(data.iloc[idx:idx+seq_len].values)\n",
    "            un_labels.append(labels.iloc[idx:idx+seq_len].values)\n",
    "            idx += stride\n",
    "        return np.array(un_data), np.array(un_labels)\n",
    "    \n",
    "    def get_root_causes(self):\n",
    "        return self.causes\n",
    "    \n",
    "class Datasettings:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataset_training_name=\"/home/jupyter/data/wadi/WADI_14days.csv\"\n",
    "        self.dataset_test_name=\"/home/jupyter/data/wadi/WADI_attackdata.csv\"\n",
    "        self.dataset_anomaly_name=\"/home/jupyter/data/wadi/WADI_anomalies.csv\"\n",
    "        self.train=True\n",
    "        self.window_length=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc396a8-38d4-4249-9b71-24e83766b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "data_settings = Datasettings()\n",
    "wadi = Wadi(seed,data_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13a673c3-6fb3-490b-bee7-ced99ac55120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209601, 126)\n",
      "Total Number of anomalies in train set = 0\n",
      "Total Number of anomalies in test set = 9948\n",
      "% of anomalies in the test set = 5.7569111289865225\n",
      "number of anomalous events = 14\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = wadi.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a4f21c-ae7d-4b2f-9ed7-aa3e87a31f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(x_train)\n",
    "x_train = imputer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03b6c5e-ae39-4be1-b8f5-ce6b4e015414",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "pca.fit(x_train)\n",
    "x_train=pca.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d619542-8528-4073-b1e7-c58f2f1c1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "pca.fit(x_test)\n",
    "x_test=pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8893e570-7b6f-4c41-8c87-3be0a2249633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_sub_seqs(x_arr, y_arr, seq_len, stride=1, start_discont=np.array([])):\n",
    "    \"\"\"\n",
    "    :param start_discont: the start points of each sub-part in case the x_arr is just multiple parts joined together\n",
    "    :param x_arr: dim 0 is time, dim 1 is channels\n",
    "    :param seq_len: size of window used to create subsequences from the data\n",
    "    :param stride: number of time points the window will move between two subsequences\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    excluded_starts = []\n",
    "    [excluded_starts.extend(range((start - seq_len + 1), start)) for start in start_discont if start > seq_len]\n",
    "    seq_starts = np.delete(np.arange(0, x_arr.shape[0] - seq_len + 1, stride), excluded_starts)\n",
    "    x_seqs = np.array([x_arr[i:i + seq_len] for i in seq_starts])\n",
    "    y_seqs=  np.array([y_arr[i:i + seq_len] for i in seq_starts])\n",
    "    y=torch.from_numpy(np.array([1 if sum(y_i) > 0 else 0 for y_i in y_seqs])).float()\n",
    "    return x_seqs,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f52091d-6fe2-47ad-98c6-94112a4428b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,y_train_per_window = get_sub_seqs(x_train,y_train, seq_len=60, stride=1,\n",
    "                                 start_discont=np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb1c4fa-97f6-4aa2-b095-f54a6a3bbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from torch.utils.data import DataLoader\n",
    "def get_train_data_loaders(x_seqs: np.ndarray, batch_size: int, splits: List, seed: int, shuffle: bool = False,\n",
    "    usetorch = True):\n",
    "    \"\"\"\n",
    "    Splits the train data between train, val, etc. Creates and returns pytorch data loaders\n",
    "    :param shuffle: boolean that determines whether samples are shuffled before splitting the data\n",
    "    :param seed: seed used for the random shuffling (if shuffling there is)\n",
    "    :param x_seqs: input data where each row is a sample (a sequence) and each column is a channel\n",
    "    :param batch_size: number of samples per batch\n",
    "    :param splits: list of split fractions, should sum up to 1.\n",
    "    :param usetorch: if True returns dataloaders, otherwise return datasets\n",
    "    :return: a tuple of data loaders as long as splits. If len_splits = 1, only 1 data loader is returned\n",
    "    \"\"\"\n",
    "    if np.sum(splits) != 1:\n",
    "        scale_factor = np.sum(splits)\n",
    "        splits = [fraction/scale_factor for fraction in splits]\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        x_seqs = x_seqs[np.random.permutation(len(x_seqs))]\n",
    "        np.random.seed()\n",
    "    split_points = [0]\n",
    "    for i in range(len(splits)-1):\n",
    "        split_points.append(split_points[-1] + int(splits[i]*len(x_seqs)))\n",
    "    split_points.append(len(x_seqs))\n",
    "    if usetorch:\n",
    "        loaders = tuple([DataLoader(dataset=x_seqs[split_points[i]:split_points[i+1]], batch_size=batch_size,\n",
    "            drop_last=False, pin_memory=True, shuffle=False) for i in range(len(splits))])\n",
    "        return loaders\n",
    "    else:\n",
    "        # datasets = tuple([x_seqs[split_points[i]: \n",
    "        #     (split_points[i] + (split_points[i+1]-split_points[i])//batch_size*batch_size)] \n",
    "        #     for i in range(len(splits))])\n",
    "        datasets = tuple([x_seqs[split_points[i]:split_points[i+1]]\n",
    "            for i in range(len(splits))])\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edeebfa2-f57d-41a1-b29b-28c807bfe120",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_val_loader = get_train_data_loaders(sequences, batch_size=32,\n",
    "                                                                splits=[1 - 0.5,\n",
    "                                                                        0.5], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb1b686-2104-43c9-9d5f-8681eeb82000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d1b0fd0-857e-447e-a81e-90dc059f7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTrn:\n",
    "    workers=4\n",
    "    batch_size=32\n",
    "    epochs=5\n",
    "    lr=0.0002\n",
    "    cuda = True\n",
    "    manualSeed=2\n",
    "    \n",
    "opt_trn=ArgsTrn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e1517a-3e63-4fa6-9da6-352d0a10b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca63aa0-bbfe-4f99-b773-b9673fc31704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) \n",
      "[GCC 9.3.0]\n",
      "__pyTorch VERSION: 1.10.0a0+3fd9dcf\n",
      "__CUDA VERSION\n",
      "/bin/bash: /libraries/Default_env_55311/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Wed_Jul_14_19:41:19_PDT_2021\n",
      "Cuda compilation tools, release 11.4, V11.4.100\n",
      "Build cuda_11.4.r11.4/compiler.30188945_0\n",
      "__CUDNN VERSION: 8202\n",
      "__Number CUDA Devices: 2\n",
      "__Devices\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, Tesla T4, 450.172.01, 15109 MiB, 1252 MiB, 13857 MiB\n",
      "1, Tesla T4, 450.172.01, 15109 MiB, 1242 MiB, 13867 MiB\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da4dc87b-9259-4d6d-b94c-009dfe823ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "embedding_dim=16\n",
    "#device = torch.device(\"cuda:0\" if opt_trn.cuda else \"cpu\") # select the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "seq_len = wadi.window_length # sequence length is equal to the window length\n",
    "in_dim = 3 # input dimension is same as number of feature\n",
    "n_features=3\n",
    "criticX = Critic(in_dim=in_dim,device=device)\n",
    "criticX=nn.DataParallel(criticX)\n",
    "criticX=criticX.to(device)\n",
    "optimizerCriticX = optim.Adam(criticX.parameters() , lr=opt_trn.lr)\n",
    "decoder=Decoder(in_dim=in_dim, out_dim=in_dim,device=device)\n",
    "decoder=nn.DataParallel(decoder)\n",
    "decoder=decoder.to(device)\n",
    "optimizerDecoder = optim.Adam(decoder.parameters(), lr=opt_trn.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b7cf7a9-5a2c-475a-b00c-483007714f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Discriminator Architecture|\n",
      " DataParallel(\n",
      "  (module): Critic(\n",
      "    (lstm): LSTM(3, 100, batch_first=True)\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "|Generator Architecture|\n",
      " DataParallel(\n",
      "  (module): Decoder(\n",
      "    (lstm0): LSTM(3, 32, batch_first=True)\n",
      "    (lstm1): LSTM(32, 64, batch_first=True)\n",
      "    (lstm2): LSTM(64, 128, batch_first=True)\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=3, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"|Discriminator Architecture|\\n\", criticX)\n",
    "print(\"|Generator Architecture|\\n\", decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf3a9a6-c61e-4de2-8c21-568802a788b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Wasserstein(nn.Module):\n",
    "    def forward(self, pred_real, pred_fake=None):\n",
    "        if pred_fake is not None:\n",
    "            loss_real = pred_real.mean()\n",
    "            loss_fake = pred_fake.mean()\n",
    "            loss = -loss_real + loss_fake\n",
    "            return loss, loss_real, loss_fake\n",
    "        else:\n",
    "            loss = -pred_real.mean()\n",
    "            return loss\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def forward(self, pred_real, pred_fake=None):\n",
    "        if pred_fake is not None:\n",
    "            loss_real = F.relu(1 - pred_real).mean()\n",
    "            loss_fake = F.relu(1 + pred_fake).mean()\n",
    "            loss = loss_real + loss_fake\n",
    "            return loss, loss_real, loss_fake\n",
    "        else:\n",
    "            loss = -pred_real.mean()\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f97a-3be6-463f-a630-719d9afd7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3257/143140855.py:29: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss -1.9232605569307448\n",
      "Epoch 1: train loss -1.9983987408062767\n",
      "Epoch 2: train loss -1.9993890828152812\n",
      "Epoch 3: train loss -1.9996577859770055\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "def normalize_gradient(net_D, x, **kwargs):\n",
    "    \"\"\"\n",
    "                     f\n",
    "    f_hat = --------------------\n",
    "            || grad_f || + | f |\n",
    "    \"\"\"\n",
    "    x.requires_grad_(True)\n",
    "    f = net_D(x, **kwargs)\n",
    "    grad = torch.autograd.grad(\n",
    "        f, [x], torch.ones_like(f), create_graph=True, retain_graph=True)[0]\n",
    "    grad_norm = torch.norm(torch.flatten(grad, start_dim=2), p=2, dim=2)\n",
    "    grad_norm = grad_norm.view(-1, *[1 for _ in range(len(f.shape) - 1)])\n",
    "    \n",
    "    f_hat = (f / (grad_norm + torch.abs(f)))\n",
    "    return f_hat\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "\n",
    "loss_fn=Wasserstein()\n",
    "for epoch in range(opt_trn.epochs):\n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        temp_train_losses = []\n",
    "        batch_size, seq_len =x.shape[0],x.shape[1] \n",
    "        x=x.float().to(device)\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1))\n",
    "        noise=noise.to(device)\n",
    "        optimizerCriticX.zero_grad()\n",
    "        for k in range(5):\n",
    "            with torch.no_grad():\n",
    "                outputOfDecoder,_ = decoder.forward(noise)\n",
    "                #outputOfDecoder=outputOfDecoder.detach()\n",
    "                outputOfDecoder=outputOfDecoder.to(device)\n",
    "            real_fake = torch.cat([x, outputOfDecoder], dim=0)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                pred = normalize_gradient(criticX, real_fake)\n",
    "                pred_real, pred_fake = torch.split(pred, [x.shape[0]*x.shape[1], outputOfDecoder.shape[0]*outputOfDecoder.shape[1]])\n",
    "                loss, loss_real, loss_fake = loss_fn(pred_real, pred_fake)\n",
    "                loss.backward()    \n",
    "                optimizerCriticX.step()\n",
    "            temp_train_losses.append(loss.item())\n",
    "        train_losses.append(np.mean(temp_train_losses))\n",
    "    train_loss = np.mean(train_losses)\n",
    "    history['train'].append(train_loss)    \n",
    "    print(f'Epoch {epoch}: train loss {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e606a7-c4d9-47bd-8c1d-5a05ed6525e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criticX.save_weights('/home/jupyter/src/saved_models/wadi/criticZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead7cd6-96e6-4244-9338-e3d32a0bd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22bd23-b457-487b-8684-317f7236f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "criticZ = Critic(in_dim=in_dim,device=device)\n",
    "criticZ=nn.DataParallel(criticZ)\n",
    "criticZ=criticZ.to(device)\n",
    "optimizerCriticZ = optim.Adam(criticZ.parameters() , lr=opt_trn.lr)\n",
    "encoder=Encoder(n_features=in_dim, embedding_dim=in_dim,device=device)\n",
    "encoder=nn.DataParallel(encoder)\n",
    "encoder=encoder.to(device)\n",
    "optimizerEncoder = optim.Adam(encoder.parameters(), lr=opt_trn.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f11b0-5b8f-4ccb-9614-29067e50d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dict(train=[], val=[])\n",
    "\n",
    "for epoch in range(opt_trn.epochs):\n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        temp_train_losses = []\n",
    "        batch_size, seq_len =x.shape[0],x.shape[1] \n",
    "        x=x.float().to(device)\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1))\n",
    "        noise=noise.to(device)\n",
    "        optimizerCriticZ.zero_grad()\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                outputOfEncoder,_ = encoder.forward(x)\n",
    "                outputOfEncoder=outputOfEncoder.detach()\n",
    "                outputOfEncoder=outputOfEncoder.to(device)\n",
    "            real_fake = torch.cat([noise, outputOfEncoder], dim=0)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                pred = normalize_gradient(criticZ, real_fake)\n",
    "                pred_real, pred_fake = torch.split(pred, [x.shape[0]*x.shape[1], outputOfEncoder.shape[0]*outputOfEncoder.shape[1]])\n",
    "                loss, loss_real, loss_fake = loss_fn(pred_real, pred_fake)\n",
    "                loss.backward()    \n",
    "                optimizerCriticZ.step()\n",
    "            temp_train_losses.append(loss.item())\n",
    "        train_losses.append(np.mean(temp_train_losses))\n",
    "    train_loss = np.mean(train_losses)\n",
    "    history['train'].append(train_loss)    \n",
    "    print(f'Epoch {epoch}: train loss {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc68765-fcbe-4e3d-883b-713b84e92228",
   "metadata": {},
   "outputs": [],
   "source": [
    "criticZ.save_weights('/home/jupyter/src/saved_models/wadi/criticZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbb6fb-2a17-4060-81b4-d12ad592cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520da683-1abc-44d0-b590-05eb940653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_normalize_gradient(net_D, x, **kwargs):\n",
    "    \"\"\"\n",
    "                     f\n",
    "    f_hat = --------------------\n",
    "            || grad_f || + | f |\n",
    "    \"\"\"\n",
    "    x.requires_grad_(True)\n",
    "    f = net_D(x, **kwargs)\n",
    "    grad = torch.autograd.grad(\n",
    "        f, [x], torch.ones_like(f), create_graph=True, retain_graph=True)[0]\n",
    "    grad_norm = torch.norm(torch.flatten(grad, start_dim=2), p=2, dim=2)\n",
    "    grad_norm = grad_norm.view(-1, *[1 for _ in range(len(f.shape) - 1)])\n",
    "    \n",
    "    f_hat = (f / (grad_norm + torch.abs(f)))\n",
    "    return f_hat\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "for epoch in range(opt_trn.epochs):\n",
    "#for epoch in range(2):\n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        batch_size, seq_len =x.shape[0],x.shape[1]\n",
    "        x=x.float().to(device)\n",
    "        optimizerDecoder.zero_grad()\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1))\n",
    "        noise=noise.to(device)\n",
    "        outputOfDecoder,_ = decoder.forward(noise)\n",
    "        outputOfDecoder=outputOfDecoder.detach()\n",
    "        outputOfDecoder=outputOfDecoder.to(device)\n",
    "        with torch.backends.cudnn.flags(enabled=False):\n",
    "            pred1 = normalize_gradient(criticX, outputOfDecoder)\n",
    "            enc_z,_=encoder.forward(x)\n",
    "            dec_x,_=decoder.forward(enc_z)\n",
    "\n",
    "        mse1=mse_loss(x,dec_x)\n",
    "        loss1=loss_fn(pred1)+mse1\n",
    "        optimizerEncoder.zero_grad()\n",
    "        \n",
    "        outputOfEncoder,_ = encoder.forward(x)\n",
    "        outputOfEncoder=outputOfEncoder.detach()\n",
    "        outputOfEncoder=outputOfEncoder.to(device)\n",
    "        with torch.backends.cudnn.flags(enabled=False):\n",
    "            pred2 = normalize_gradient(criticZ, outputOfEncoder)\n",
    "            dec_x,_=decoder.forward(noise)\n",
    "            enc_z,_=encoder.forward(dec_x)\n",
    "        \n",
    "        mse2=mse_loss(noise,enc_z)\n",
    "        loss2=loss_fn(pred2)+mse2\n",
    "        err=loss1+loss2\n",
    "        err.backward()\n",
    "        optimizerDecoder.step()\n",
    "        optimizerEncoder.step()\n",
    "        \n",
    "        train_losses.append(err.item())\n",
    "    train_loss = np.mean(train_losses)\n",
    "    history['train'].append(train_loss)\n",
    "    print(f'Epoch {epoch}: train loss {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5a9cd-4761-4c1b-aa19-ee473e99e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('/home/jupyter/src/saved_models/wadi/encoder')\n",
    "decoder.save_weights('/home/jupyter/src/saved_models/wadi/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d817c0-f8b5-410c-bde7-c6c9b861a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,y_test_per_window = get_sub_seqs(x_test,y_test, seq_len=60, stride=60, start_discont=np.array([]))\n",
    "test_loader = DataLoader(dataset=sequences, batch_size=1,  num_workers=4,\n",
    "                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214236de-15e6-4348-a554-7a8543bf341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.tf_dtw import SoftDTW\n",
    "criterion_dtw = SoftDTW(gamma=1.0, normalize=True) # just like nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "criterion =  nn.L1Loss(reduction='sum')\n",
    "for x in test_loader:\n",
    "    x=x.float().to(device)\n",
    "    enc_z,_=encoder.forward(x)\n",
    "    enc_z=enc_z.to(device)\n",
    "    dec_x,_=decoder.forward(enc_z)\n",
    "    dec_x=dec_x.to(device)\n",
    "    err1=criterion(x,dec_x)\n",
    "    \n",
    "    distance1=criterion_dtw(x, dec_x)\n",
    "    pred1=criticX.forward(x)\n",
    "    criticx_loss=loss_fn(pred1)\n",
    "    \n",
    "    enc_x,_=decoder.forward(enc_z)\n",
    "    enc_x=enc_x.to(device)\n",
    "    dec_enc_z,_=encoder.forward(enc_x)\n",
    "    dec_enc_z=dec_enc_z.to(device)\n",
    "    \n",
    "    pred2=criticZ.forward(enc_z)\n",
    "    criticz_loss=loss_fn(pred2)\n",
    "    \n",
    "    err2=criterion(enc_z,dec_enc_z)\n",
    "    distance2=criterion_dtw(enc_z, dec_enc_z)\n",
    "    err=err1+err2+criticz_loss+criticx_loss\n",
    "    losses.append(err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe06fcb-4ad7-4e1e-b906-c5cc0b8a9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b85e3-4741-46a0-a023-dbbe54246783",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9), dpi=80)\n",
    "plt.title('Loss Distribution', fontsize=16)\n",
    "sns.histplot(losses, bins = 20, kde= False, color = 'blue');\n",
    "#sns.distplot(losses, bins = 20, kde= True, color = 'blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb4519-9c07-4f27-a296-fe9abd051a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD =6.59\n",
    "\n",
    "test_score_df = pd.DataFrame(index=range(len(losses)))\n",
    "test_score_df['loss'] = [loss/60 for loss in losses]\n",
    "test_score_df['y'] = y_test_per_window\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "#test_score_df['t'] = [x[59].item() for x in sequences]\n",
    "\n",
    "plt.plot( test_score_df.loss, label='loss')\n",
    "plt.plot( test_score_df.threshold, label='threshold')\n",
    "plt.plot( test_score_df.y, label='y')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd70489-6afa-4784-b83e-dd569ee98c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import seaborn as sns\n",
    "\n",
    "anomalies = test_score_df[test_score_df.anomaly == True]\n",
    "\n",
    "plt.plot(\n",
    "  test_score_df['t'], \n",
    "  label='value'\n",
    ");\n",
    "\n",
    "sns.scatterplot(\n",
    " \n",
    "  anomalies.t,\n",
    "  color=sns.color_palette()[3],\n",
    "  s=52,\n",
    "  label='anomaly'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "  test_score_df['y'],\n",
    "  label='y'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ac591-585f-463f-9b3b-a2040bbb7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_end = []\n",
    "state = 0\n",
    "for idx in test_score_df.index:\n",
    "    if state==0 and test_score_df.loc[idx, 'y']==1:\n",
    "        state=1\n",
    "        start = idx\n",
    "    if state==1 and test_score_df.loc[idx, 'y']==0:\n",
    "        state = 0\n",
    "        end = idx\n",
    "        start_end.append((start, end))\n",
    "\n",
    "for s_e in start_end:\n",
    "    if sum(test_score_df[s_e[0]:s_e[1]+1]['anomaly'])>0:\n",
    "        for i in range(s_e[0], s_e[1]+1):\n",
    "            test_score_df.loc[i, 'anomaly'] = 1\n",
    "            \n",
    "actual = np.array(test_score_df['y'])\n",
    "predicted = np.array([int(a) for a in test_score_df['anomaly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c980c63-a6a5-44a2-8c06-27c6f14009a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "predicted = np.array(predicted)\n",
    "actual = np.array(actual)\n",
    "\n",
    "tp = np.count_nonzero(predicted * actual)\n",
    "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp = np.count_nonzero(predicted * (actual - 1))\n",
    "fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "print('True Positive\\t', tp)\n",
    "print('True Negative\\t', tn)\n",
    "print('False Positive\\t', fp)\n",
    "print('False Negative\\t', fn)\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "print('Accuracy\\t', accuracy)\n",
    "print('Precision\\t', precision)\n",
    "print('Recall\\t', recall)\n",
    "print('f-measure\\t', fmeasure)\n",
    "print('cohen_kappa_score\\t', cohen_kappa_score)\n",
    "print('auc\\t', auc_val)\n",
    "print('roc_auc\\t', roc_auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafefb90-b5bc-47d1-a6ad-50a639507469",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014d427-925b-4866-a965-5f922468bda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf2387-29f0-4aad-aea2-d57ffb6cfdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
